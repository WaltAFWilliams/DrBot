{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2 \n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential, load_model, save_model\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications import *\n",
    "from efficientnet.tfkeras import EfficientNetB7\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General settings\n",
    "BATCH_SIZE = 100\n",
    "IMG_SIZE = 224\n",
    "auto = tf.data.experimental.AUTOTUNE\n",
    "n_epochs = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13_left</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13_right</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15_left</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      image  level\n",
       "0   10_left      0\n",
       "1  10_right      0\n",
       "2   13_left      0\n",
       "3  13_right      0\n",
       "4   15_left      1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"trainLabels_cropped.csv\", index_col=False).drop([\"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               image level\n",
       "0  ./resized_train_cropped/resized_train_cropped/...     0\n",
       "1  ./resized_train_cropped/resized_train_cropped/...     0\n",
       "2  ./resized_train_cropped/resized_train_cropped/...     0\n",
       "3  ./resized_train_cropped/resized_train_cropped/...     0\n",
       "4  ./resized_train_cropped/resized_train_cropped/...     1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_dir = \"./resized_train_cropped/resized_train_cropped/\"\n",
    "df[\"image\"] = df[\"image\"].apply(lambda pic: img_dir+pic+\".jpeg\")\n",
    "df[\"level\"] = df[\"level\"].apply(lambda num: str(num))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = df[\"image\"].values\n",
    "labels = df[\"level\"].values\n",
    "\n",
    "train_paths, test_paths, train_labels, test_labels = train_test_split(paths, labels, \n",
    "                                                                      test_size=0.33, random_state=1996)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23517</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23518</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23519</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23520</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23521</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23522 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path label\n",
       "0      ./resized_train_cropped/resized_train_cropped/...     0\n",
       "1      ./resized_train_cropped/resized_train_cropped/...     1\n",
       "2      ./resized_train_cropped/resized_train_cropped/...     2\n",
       "3      ./resized_train_cropped/resized_train_cropped/...     0\n",
       "4      ./resized_train_cropped/resized_train_cropped/...     2\n",
       "...                                                  ...   ...\n",
       "23517  ./resized_train_cropped/resized_train_cropped/...     2\n",
       "23518  ./resized_train_cropped/resized_train_cropped/...     1\n",
       "23519  ./resized_train_cropped/resized_train_cropped/...     0\n",
       "23520  ./resized_train_cropped/resized_train_cropped/...     0\n",
       "23521  ./resized_train_cropped/resized_train_cropped/...     0\n",
       "\n",
       "[23522 rows x 2 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_paths_df = pd.DataFrame(train_paths, columns=[\"path\"])\n",
    "t_labels_df = pd.DataFrame(train_labels, columns=[\"label\"])\n",
    "train_df = pd.concat([t_paths_df, t_labels_df], axis=1)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11581</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11582</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11583</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11584</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11585</th>\n",
       "      <td>./resized_train_cropped/resized_train_cropped/...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11586 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    path label\n",
       "0      ./resized_train_cropped/resized_train_cropped/...     0\n",
       "1      ./resized_train_cropped/resized_train_cropped/...     0\n",
       "2      ./resized_train_cropped/resized_train_cropped/...     0\n",
       "3      ./resized_train_cropped/resized_train_cropped/...     0\n",
       "4      ./resized_train_cropped/resized_train_cropped/...     0\n",
       "...                                                  ...   ...\n",
       "11581  ./resized_train_cropped/resized_train_cropped/...     1\n",
       "11582  ./resized_train_cropped/resized_train_cropped/...     0\n",
       "11583  ./resized_train_cropped/resized_train_cropped/...     0\n",
       "11584  ./resized_train_cropped/resized_train_cropped/...     2\n",
       "11585  ./resized_train_cropped/resized_train_cropped/...     0\n",
       "\n",
       "[11586 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_paths_df = pd.DataFrame(test_paths, columns=[\"path\"])\n",
    "t_labels_df = pd.DataFrame(test_labels, columns=[\"label\"])\n",
    "test_df = pd.concat([t_paths_df, t_labels_df], axis=1)\n",
    "#test_df[\"label\"] = test_df[\"label\"].astype(int)\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.addWeighted(img, 4, cv2.GaussianBlur(img, (0,0), 10), -4, 128)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = ImageDataGenerator(horizontal_flip=True, vertical_flip=True, rotation_range=360,\n",
    "                              rescale=(1/255.), preprocessing_function=preprocess_data, validation_split=0.15,\n",
    "                              )\n",
    "test_gen = ImageDataGenerator(rescale=(1/255.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 19994 validated image filenames belonging to 5 classes.\n",
      "Found 3528 validated image filenames belonging to 5 classes.\n",
      "Found 11586 validated image filenames belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "training_gen = train_gen.flow_from_dataframe(train_df, x_col=\"path\", y_col=\"label\", target_size=(224,224),\n",
    "                                            class_mode=\"sparse\", batch_size=BATCH_SIZE, subset=\"training\")\n",
    "\n",
    "valid_gen = train_gen.flow_from_dataframe(train_df, x_col=\"path\", y_col=\"label\", target_size=(224,224),\n",
    "                                            class_mode=\"sparse\", batch_size=BATCH_SIZE, subset=\"validation\")\n",
    "\n",
    "test_gen = test_gen.flow_from_dataframe(test_df, x_col=\"path\", y_col=\"label\", target_size=(224,224),\n",
    "                                        class_mode=\"sparse\", batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(which=\"mobilenet\", dense=False, img_size=(IMG_SIZE,IMG_SIZE,3)):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    dense - (bool) If True, makes the top layer (classifier) a fully-connected dense layer (default). If False the top layer is a global average pooling layer.\n",
    "\n",
    "    which - (String) {Options:  mobilenet=MobileNetV2(default), incepresnet=InceptionResNetV2, densenet=DenseNet121, effnet=EfficientNetB7} (case-insensitive) which model to use as the base classifier\n",
    "\n",
    "    img_size - (Tuple) Size of input images\n",
    "        \n",
    "    Returns a model\n",
    "    \n",
    "    \"\"\"\n",
    "    # Choosing which model to use as our base\n",
    "    which = which.lower()\n",
    "    \n",
    "    if which==\"incepresnet\":\n",
    "        base = InceptionResNetV2(input_shape=(img_size), include_top=False, weights=\"imagenet\")\n",
    "    elif which==\"densenet\":\n",
    "        base = DenseNet121(input_shape=(img_size), include_top=False, weights=\"imagenet\")\n",
    "    elif which==\"effnet\":\n",
    "        base = EfficientNetB7(input_shape=(img_size), include_top=False, weights=\"imagenet\")\n",
    "    else: \n",
    "        base = MobileNetV2(input_shape=(img_size), include_top=False, weights=\"imagenet\")\n",
    "    \n",
    "    # Choosing the top for our model\n",
    "    if dense:\n",
    "        flatten = Flatten()\n",
    "        fc1 = Dense(512, activation=\"relu\")\n",
    "        fc2 = Dense(BATCH_SIZE, activation=\"relu\")\n",
    "        model = Sequential([\n",
    "            base,\n",
    "            flatten,\n",
    "            fc1,\n",
    "            fc2\n",
    "        ])\n",
    "    else:\n",
    "        GAP = GlobalAveragePooling2D()\n",
    "        dropout = Dropout(0.5)\n",
    "        model = Sequential([base, dropout, GAP])\n",
    "    \n",
    "    output = Dense(5, activation=\"softmax\")\n",
    "    model.add(output)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(.0005)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "    \n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[\"acc\"])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters and callbacks\n",
    "STEPS = len(train_paths) // (4*BATCH_SIZE)\n",
    "reduce_lr  = tf.keras.callbacks.ReduceLROnPlateau(patience=5, verbose=1)\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True)\n",
    "\n",
    "# Setting up class weights\n",
    "freq = [(1 - df[\"level\"].value_counts()[i] / len(df)) for i in range(5)]\n",
    "class_weights = {\n",
    "    0: freq[0],\n",
    "    1: freq[1],\n",
    "    2: freq[2],\n",
    "    3: freq[3],\n",
    "    4: freq[4]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "mobilenetv2_1.00_224 (Model) (None, 7, 7, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 7, 7, 1280)        0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 6405      \n",
      "=================================================================\n",
      "Total params: 2,264,389\n",
      "Trainable params: 2,230,277\n",
      "Non-trainable params: 34,112\n",
      "_________________________________________________________________\n",
      "Epoch 1/600\n",
      "200/200 [==============================] - 681s 3s/step - loss: 0.5117 - acc: 0.6355 - val_loss: 1.5344 - val_acc: 0.5377 - lr: 5.0000e-04\n",
      "Epoch 2/600\n",
      "200/200 [==============================] - 431s 2s/step - loss: 0.4454 - acc: 0.7243 - val_loss: 2.1666 - val_acc: 0.4974 - lr: 5.0000e-04\n",
      "Epoch 3/600\n",
      "200/200 [==============================] - 433s 2s/step - loss: 0.4268 - acc: 0.7371 - val_loss: 4.4597 - val_acc: 0.2174 - lr: 5.0000e-04\n",
      "Epoch 4/600\n",
      "200/200 [==============================] - 433s 2s/step - loss: 0.4153 - acc: 0.7503 - val_loss: 2.2436 - val_acc: 0.4430 - lr: 5.0000e-04\n",
      "Epoch 5/600\n",
      "200/200 [==============================] - 433s 2s/step - loss: 0.4029 - acc: 0.7572 - val_loss: 2.3178 - val_acc: 0.4467 - lr: 5.0000e-04\n",
      "Epoch 6/600\n",
      "200/200 [==============================] - 438s 2s/step - loss: 0.3939 - acc: 0.7626 - val_loss: 0.9561 - val_acc: 0.7046 - lr: 5.0000e-04\n",
      "Epoch 7/600\n",
      "200/200 [==============================] - 442s 2s/step - loss: 0.3907 - acc: 0.7675 - val_loss: 1.6750 - val_acc: 0.5692 - lr: 5.0000e-04\n",
      "Epoch 8/600\n",
      "200/200 [==============================] - 440s 2s/step - loss: 0.3882 - acc: 0.7672 - val_loss: 3.6240 - val_acc: 0.2778 - lr: 5.0000e-04\n",
      "Epoch 9/600\n",
      "200/200 [==============================] - 453s 2s/step - loss: 0.3816 - acc: 0.7723 - val_loss: 7.3734 - val_acc: 0.1066 - lr: 5.0000e-04\n",
      "Epoch 10/600\n",
      "200/200 [==============================] - 459s 2s/step - loss: 0.3762 - acc: 0.7753 - val_loss: 8.1899 - val_acc: 0.0417 - lr: 5.0000e-04\n",
      "Epoch 11/600\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.3691 - acc: 0.7774\n",
      "Epoch 00011: ReduceLROnPlateau reducing learning rate to 5.0000002374872565e-05.\n",
      "200/200 [==============================] - 459s 2s/step - loss: 0.3691 - acc: 0.7774 - val_loss: 3.9499 - val_acc: 0.2251 - lr: 5.0000e-04\n",
      "Epoch 12/600\n",
      "200/200 [==============================] - 463s 2s/step - loss: 0.3468 - acc: 0.7894 - val_loss: 3.2519 - val_acc: 0.2557 - lr: 5.0000e-05\n",
      "Epoch 13/600\n",
      "200/200 [==============================] - 463s 2s/step - loss: 0.3368 - acc: 0.7928 - val_loss: 1.6801 - val_acc: 0.4405 - lr: 5.0000e-05\n",
      "Epoch 14/600\n",
      "200/200 [==============================] - 460s 2s/step - loss: 0.3299 - acc: 0.7972 - val_loss: 1.0751 - val_acc: 0.5811 - lr: 5.0000e-05\n",
      "Epoch 15/600\n",
      "200/200 [==============================] - 461s 2s/step - loss: 0.3286 - acc: 0.7956 - val_loss: 0.9716 - val_acc: 0.6270 - lr: 5.0000e-05\n",
      "Epoch 16/600\n",
      "200/200 [==============================] - 460s 2s/step - loss: 0.3219 - acc: 0.7965 - val_loss: 0.8624 - val_acc: 0.6678 - lr: 5.0000e-05\n",
      "Epoch 17/600\n",
      "200/200 [==============================] - 459s 2s/step - loss: 0.3195 - acc: 0.7965 - val_loss: 0.7955 - val_acc: 0.7058 - lr: 5.0000e-05\n",
      "Epoch 18/600\n",
      "200/200 [==============================] - 450s 2s/step - loss: 0.3120 - acc: 0.7991 - val_loss: 0.9316 - val_acc: 0.6380 - lr: 5.0000e-05\n",
      "Epoch 19/600\n",
      "200/200 [==============================] - 450s 2s/step - loss: 0.3107 - acc: 0.7978 - val_loss: 0.7838 - val_acc: 0.7262 - lr: 5.0000e-05\n",
      "Epoch 20/600\n",
      "200/200 [==============================] - 451s 2s/step - loss: 0.3069 - acc: 0.8039 - val_loss: 0.8724 - val_acc: 0.6658 - lr: 5.0000e-05\n",
      "Epoch 21/600\n",
      "200/200 [==============================] - 450s 2s/step - loss: 0.3018 - acc: 0.8028 - val_loss: 0.7399 - val_acc: 0.7500 - lr: 5.0000e-05\n",
      "Epoch 22/600\n",
      "200/200 [==============================] - 450s 2s/step - loss: 0.2962 - acc: 0.8092 - val_loss: 0.7808 - val_acc: 0.7075 - lr: 5.0000e-05\n",
      "Epoch 23/600\n",
      "200/200 [==============================] - 451s 2s/step - loss: 0.2926 - acc: 0.8070 - val_loss: 0.6846 - val_acc: 0.7645 - lr: 5.0000e-05\n",
      "Epoch 24/600\n",
      "200/200 [==============================] - 450s 2s/step - loss: 0.2870 - acc: 0.8066 - val_loss: 0.7343 - val_acc: 0.7520 - lr: 5.0000e-05\n",
      "Epoch 25/600\n",
      "200/200 [==============================] - 449s 2s/step - loss: 0.2833 - acc: 0.8067 - val_loss: 0.6598 - val_acc: 0.7829 - lr: 5.0000e-05\n",
      "Epoch 26/600\n",
      "200/200 [==============================] - 449s 2s/step - loss: 0.2788 - acc: 0.8082 - val_loss: 0.6931 - val_acc: 0.7596 - lr: 5.0000e-05\n",
      "Epoch 27/600\n",
      "200/200 [==============================] - 459s 2s/step - loss: 0.2749 - acc: 0.8138 - val_loss: 0.6773 - val_acc: 0.7798 - lr: 5.0000e-05\n",
      "Epoch 28/600\n",
      "200/200 [==============================] - 457s 2s/step - loss: 0.2665 - acc: 0.8141 - val_loss: 0.6561 - val_acc: 0.7928 - lr: 5.0000e-05\n",
      "Epoch 29/600\n",
      "200/200 [==============================] - 454s 2s/step - loss: 0.2646 - acc: 0.8145 - val_loss: 0.6656 - val_acc: 0.7761 - lr: 5.0000e-05\n",
      "Epoch 30/600\n",
      "200/200 [==============================] - 454s 2s/step - loss: 0.2568 - acc: 0.8157 - val_loss: 0.6880 - val_acc: 0.7710 - lr: 5.0000e-05\n",
      "Epoch 31/600\n",
      "200/200 [==============================] - 450s 2s/step - loss: 0.2552 - acc: 0.8217 - val_loss: 0.6608 - val_acc: 0.7900 - lr: 5.0000e-05\n",
      "Epoch 32/600\n",
      "200/200 [==============================] - 449s 2s/step - loss: 0.2485 - acc: 0.8242 - val_loss: 0.6663 - val_acc: 0.7905 - lr: 5.0000e-05\n",
      "Epoch 33/600\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.2391 - acc: 0.8273\n",
      "Epoch 00033: ReduceLROnPlateau reducing learning rate to 5.000000237487257e-06.\n",
      "200/200 [==============================] - 450s 2s/step - loss: 0.2391 - acc: 0.8273 - val_loss: 0.6563 - val_acc: 0.7968 - lr: 5.0000e-05\n",
      "Epoch 34/600\n",
      "200/200 [==============================] - 455s 2s/step - loss: 0.2295 - acc: 0.8320 - val_loss: 0.6509 - val_acc: 0.7928 - lr: 5.0000e-06\n",
      "Epoch 35/600\n",
      "200/200 [==============================] - 461s 2s/step - loss: 0.2254 - acc: 0.8347 - val_loss: 0.6590 - val_acc: 0.7962 - lr: 5.0000e-06\n",
      "Epoch 36/600\n",
      "200/200 [==============================] - 461s 2s/step - loss: 0.2229 - acc: 0.8393 - val_loss: 0.6574 - val_acc: 0.7803 - lr: 5.0000e-06\n",
      "Epoch 37/600\n",
      "200/200 [==============================] - 463s 2s/step - loss: 0.2230 - acc: 0.8369 - val_loss: 0.6785 - val_acc: 0.7764 - lr: 5.0000e-06\n",
      "Epoch 38/600\n",
      "200/200 [==============================] - 461s 2s/step - loss: 0.2207 - acc: 0.8389 - val_loss: 0.6786 - val_acc: 0.7778 - lr: 5.0000e-06\n",
      "Epoch 39/600\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.2174 - acc: 0.8435\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 5.000000328436726e-07.\n",
      "200/200 [==============================] - 461s 2s/step - loss: 0.2174 - acc: 0.8435 - val_loss: 0.6793 - val_acc: 0.7687 - lr: 5.0000e-06\n",
      "Epoch 40/600\n",
      "200/200 [==============================] - 462s 2s/step - loss: 0.2193 - acc: 0.8396 - val_loss: 0.7162 - val_acc: 0.7596 - lr: 5.0000e-07\n",
      "Epoch 41/600\n",
      "200/200 [==============================] - 461s 2s/step - loss: 0.2182 - acc: 0.8418 - val_loss: 0.7040 - val_acc: 0.7602 - lr: 5.0000e-07\n",
      "Epoch 42/600\n",
      "200/200 [==============================] - 460s 2s/step - loss: 0.2188 - acc: 0.8392 - val_loss: 0.7146 - val_acc: 0.7636 - lr: 5.0000e-07\n",
      "Epoch 43/600\n",
      "200/200 [==============================] - 459s 2s/step - loss: 0.2132 - acc: 0.8409 - val_loss: 0.7372 - val_acc: 0.7500 - lr: 5.0000e-07\n",
      "Epoch 44/600\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.2194 - acc: 0.8376\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 5.000000555810402e-08.\n",
      "200/200 [==============================] - 460s 2s/step - loss: 0.2194 - acc: 0.8376 - val_loss: 0.7449 - val_acc: 0.7537 - lr: 5.0000e-07\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/600\n",
      "200/200 [==============================] - 460s 2s/step - loss: 0.2144 - acc: 0.8402 - val_loss: 0.7281 - val_acc: 0.7594 - lr: 5.0000e-08\n",
      "Epoch 46/600\n",
      "200/200 [==============================] - 458s 2s/step - loss: 0.2150 - acc: 0.8440 - val_loss: 0.7465 - val_acc: 0.7579 - lr: 5.0000e-08\n",
      "Epoch 47/600\n",
      "200/200 [==============================] - 447s 2s/step - loss: 0.2147 - acc: 0.8424 - val_loss: 0.7551 - val_acc: 0.7477 - lr: 5.0000e-08\n",
      "Epoch 48/600\n",
      "200/200 [==============================] - 446s 2s/step - loss: 0.2169 - acc: 0.8396 - val_loss: 0.7461 - val_acc: 0.7509 - lr: 5.0000e-08\n",
      "Epoch 49/600\n",
      "200/200 [==============================] - ETA: 0s - loss: 0.2159 - acc: 0.8421\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 5.000000413701855e-09.\n",
      "200/200 [==============================] - 446s 2s/step - loss: 0.2159 - acc: 0.8421 - val_loss: 0.7528 - val_acc: 0.7491 - lr: 5.0000e-08\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffa780ca4e0>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.fit(training_gen, validation_data=valid_gen, epochs=n_epochs, batch_size=BATCH_SIZE, \n",
    "          class_weight=class_weights, callbacks=[early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"mobilenet_img_preprocess.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.argmax(model.predict(test_gen), axis=1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.80      0.78      8610\n",
      "           1       0.09      0.04      0.05       770\n",
      "           2       0.20      0.25      0.22      1721\n",
      "           3       0.30      0.05      0.08       286\n",
      "           4       0.34      0.16      0.21       199\n",
      "\n",
      "    accuracy                           0.64     11586\n",
      "   macro avg       0.34      0.26      0.27     11586\n",
      "weighted avg       0.62      0.64      0.62     11586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.91      0.88      8610\n",
      "           1       0.17      0.09      0.12       770\n",
      "           2       0.54      0.54      0.54      1721\n",
      "           3       0.56      0.28      0.37       286\n",
      "           4       0.68      0.44      0.54       199\n",
      "\n",
      "    accuracy                           0.78     11586\n",
      "   macro avg       0.56      0.45      0.49     11586\n",
      "weighted avg       0.75      0.78      0.76     11586\n",
      "\n"
     ]
    }
   ],
   "source": [
    "old = load_model(\"models/mobilenet_class_weights.h5\")\n",
    "preds = np.argmax(old.predict(test_gen), axis=1).astype(str)\n",
    "print(classification_report(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
